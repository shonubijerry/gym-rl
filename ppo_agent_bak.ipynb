{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Categorical\n",
    "from utils import transform\n",
    "from fileIO import FileIO\n",
    "from utils import file_exists, write_history, get_last_history, compute_reward, decimal2\n",
    "\n",
    "# Define the PPO actor and critic neural network\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"CNN with 3 convolution layers\"\"\"\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(512, num_actions)\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(torch.prod(torch.tensor(o.size())))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # m = x[0].permute(1, 2, 0).numpy()\n",
    "        # plt.imshow(m)\n",
    "        # plt.axis('off')  # Optional: Turn off axis ticks and labels\n",
    "        # plt.show()\n",
    "        # return 'uuu'\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        fc_out = self.fc(conv_out)\n",
    "        return self.actor(fc_out), self.critic(fc_out)\n",
    "\n",
    "\n",
    "# Proximal Policy Optimization Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, env, env_name: str, lr=0.001, gamma=0.99, clip_epsilon=0.2,\n",
    "                 value_coef=0.5, entropy_coef=0.01):\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.name = env_name.lower()\n",
    "        self.input_shape = (3, 160, 160)\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.actor_critic = ActorCritic(self.input_shape, self.num_actions).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "        \n",
    "        # Define hyperparameters\n",
    "        self.gamma = gamma\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        self.resume = False\n",
    "        self.policy_path = f'saves/{self.name}/ppo_policy.h5'\n",
    "        self.history_path = f'saves/{self.name}/ppo_history.csv'\n",
    "        self.log = ''\n",
    "        \n",
    "        # save time by retraining model from a known trained weights\n",
    "        if (file_exists(self.policy_path)):\n",
    "            self.resume = True\n",
    "            self.actor_critic.load_state_dict(torch.load(self.policy_path))\n",
    "            print('----- loaded saved weights ------')\n",
    "\n",
    "        # implement evaluation history so traning can be ran in batches to not crash system\n",
    "        self.current_episode =  get_last_history(self.history_path)\n",
    "\n",
    "    def act(self, state):\n",
    "        # state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            dist, value = self.actor_critic(transform(state))\n",
    "            logits = F.softmax(dist, dim=-1)\n",
    "            action_probs = Categorical(logits)\n",
    "            action = action_probs.sample()\n",
    "            # self.log = f'{logits}, {action}'\n",
    "            \n",
    "        return action.item(), logits.tolist()\n",
    "\n",
    "    def learn(self, rollout):\n",
    "        states, actions, old_probs, rewards, next_states, dones = rollout\n",
    "\n",
    "        actions = torch.LongTensor([actions])\n",
    "        rewards = torch.FloatTensor([rewards])\n",
    "        old_probs = torch.FloatTensor([old_probs])\n",
    "\n",
    "        dist, values = self.actor_critic(states)\n",
    "        _, next_values = self.actor_critic(next_states)\n",
    "\n",
    "        advantages = rewards + self.gamma * next_values * (1 - dones) - values.detach()\n",
    "        returns = rewards + self.gamma * next_values * (1 - dones)\n",
    "\n",
    "        prob_ratio = torch.exp(dist.squeeze(dim=0) - old_probs)\n",
    "\n",
    "        surrogate1 = prob_ratio * advantages\n",
    "        surrogate2 = torch.clamp(prob_ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "\n",
    "        actor_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "        critic_loss = nn.MSELoss()(values, returns)\n",
    "\n",
    "        # Calculate entropy using Categorical distribution\n",
    "        entropy = Categorical(probs=F.softmax(dist, dim=-1)).entropy().mean()\n",
    "\n",
    "        loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    \n",
    "    def train(self, num_episodes):\n",
    "        history_data = []\n",
    "        save_after = 100\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()[0]\n",
    "\n",
    "            done = False\n",
    "            total_trad_reward = 0\n",
    "            total_nontrad_reward = 0\n",
    "            timestep = 0\n",
    "            \n",
    "            batch_states = []\n",
    "            batch_actions = []\n",
    "            batch_log_probs = []\n",
    "            batch_rewards = []\n",
    "\n",
    "            while timestep < 10:\n",
    "                action, logits = self.act(state)\n",
    "                next_state, reward, done, _, __ = self.env.step(action)\n",
    "                \n",
    "                # get traditional and non-traditional reward\n",
    "                trad, nontrad = compute_reward(reward, done)\n",
    "                \n",
    "                combined_reward = trad + nontrad\n",
    "                total_trad_reward += trad\n",
    "                total_nontrad_reward += nontrad\n",
    "\n",
    "                self.learn((state, action, logits, combined_reward, next_state, done))\n",
    "\n",
    "                state = next_state\n",
    "                \n",
    "                done = done or _\n",
    "                timestep += 1\n",
    "\n",
    "            total_nontrad_reward = decimal2(total_nontrad_reward)\n",
    "            total_reward = total_trad_reward + total_nontrad_reward\n",
    "            \n",
    "            print(f\"Episode {episode + 1} | Total Reward: {total_reward}, Traditional: {total_trad_reward}\")\n",
    "            \n",
    "            # persist history and policy after every 20 episodes\n",
    "            history_data.append(f\"{int(self.current_episode) + episode + 1}, {total_reward}, {total_trad_reward}, {total_nontrad_reward}, {timestep}, {self.log}\")\n",
    "            if ((episode + 1) % save_after == 0):\n",
    "                # Save the network weights\n",
    "                torch.save(self.actor_critic.state_dict(), self.policy_path)\n",
    "                \n",
    "                # write history\n",
    "                write_history(self.history_path, history_data)\n",
    "                history_data = []\n",
    "\n",
    "    def evaluate(self, num_episodes):\n",
    "        wins = 0\n",
    "        for episode in range(num_episodes):\n",
    "            state = transform(self.env.reset()[0])\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.act(state)[0]\n",
    "                next_state, reward, done, _, __ = self.env.step(action)\n",
    "                state = transform(next_state)\n",
    "                if reward > 0:\n",
    "                    wins += 1\n",
    "        \n",
    "        print(f'Wins - {wins}; Episodes - {num_episodes}; Average - {wins / num_episodes}') \n",
    "    \n",
    "    def close_session(self):\n",
    "        self.env.close()\n",
    "        \n",
    "env_name = 'Breakout-v4'\n",
    "# env_name = 'Berzerk-v4'\n",
    "training_batch = 1\n",
    "training_episodes = 10\n",
    "eval_episodes = 10\n",
    "\n",
    "for i in range(training_batch):\n",
    "  print(f'---- starting environment {i + 1} ----')\n",
    "  env = gym.make(env_name)\n",
    "  env_name = env.spec.id.split('-')[0]\n",
    "  agent = PPOAgent(env, env_name)\n",
    "  agent.train(training_episodes)\n",
    "  agent.close_session()\n",
    "  \n",
    "  # free up memory for next iteration\n",
    "  del env\n",
    "  del agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATARI Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Proxy Proximal Optimization Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_name = 'Breakout-v4'\n",
    "env_name = 'Berzerk-v4'\n",
    "training_batch = 1\n",
    "training_episodes = 10\n",
    "eval_episodes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(training_batch):\n",
    "  env = gym.make(env_name)\n",
    "  env_name = env.spec.id.split('-')[0]\n",
    "  agent = PPOAgent(env, env_name)\n",
    "  agent.train(training_episodes)\n",
    "  agent.close_session()\n",
    "  \n",
    "  # free up memory for next iteration\n",
    "  del env\n",
    "  del agent\n",
    "  \n",
    "  print(f'---- starting environment {i + 1} ----')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Trained PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "env_name = env.spec.id.split('-')[0]\n",
    "agent = PPOAgent(env, env_name)\n",
    "agent.evaluate(eval_episodes)\n",
    "agent.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

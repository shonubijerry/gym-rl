{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import ReplayBuffer, label_with_episode_number, save_random_agent_gif, file_exists, write_history, get_last_history, compute_reward, decimal2,transform\n",
    "\n",
    "from environment_wrapper import EnvironmentWrapper, ObservationWrapper, RewardWrapper, ActionWrapper\n",
    "from fileIO import FileIO\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"CNN with 3 convolution layers\"\"\"\n",
    "    def __init__(self, output, name, batch_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.name = f'{name}_agent.h5'\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc = nn.Flatten()\n",
    "        self.fc2 = nn.Linear(128 * 200, output)\n",
    "    \n",
    "    def forward(self, x, batch = None):\n",
    "        # m = x[0].permute(0, 1, 2).numpy()\n",
    "        # x = x.reshape(batch or self.batch_size, 3, 210, 160)\n",
    "        # print(type(x[0]))\n",
    "        # m = x[0].permute(1, 2, 0).numpy()\n",
    "        # plt.imshow(m)\n",
    "        # plt.axis('off')  # Optional: Turn off axis ticks and labels\n",
    "        # plt.show()\n",
    "        # return 'uuu'\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.fc2(x)\n",
    "        # print(x)\n",
    "        return x\n",
    "\n",
    "class DQNAgent(object):\n",
    "    def __init__(self, env, env_name):\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.batch_size = env.action_space.n\n",
    "        self.input_shape = (3, 160, 160)\n",
    "        self.name = env_name.lower()\n",
    "        self.wrapped_env = env\n",
    "        self.action_dim = self.wrapped_env.action_space.n\n",
    "        \n",
    "        # Create the DQN model and target network\n",
    "        self.model = DQN(self.action_dim, self.name, self.batch_size).to(self.device)\n",
    "        \n",
    "        # Define hyperparameters\n",
    "        self.learning_rate = 0.001\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.epsilon_min = 0.01\n",
    "        self.target_update_freq = 10\n",
    "        self.buffer_capacity = 10000\n",
    "        \n",
    "        self.resume = False\n",
    "        self.policy_path = f'saves/{self.name}/dqn_policy.h5'\n",
    "        self.history_path = f'saves/{self.name}/dqn_history.csv'\n",
    "        self.log = ''\n",
    "\n",
    "        # save time by retraining model from a known trained weights\n",
    "        if (file_exists(self.policy_path)):\n",
    "            self.resume = True\n",
    "            self.model.load_state_dict(torch.load(self.policy_path))\n",
    "\n",
    "        self.target_model = DQN(self.action_dim, self.name, self.batch_size).to(self.device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        # self.target_model.eval()\n",
    "\n",
    "        # Define the loss function and optimizer\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Define the replay buffer\n",
    "        self.Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "        # Initialize the replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_capacity)\n",
    "\n",
    "        # implement evaluation history so traning can be ran in batches to not crash system\n",
    "        self.current_episode =  get_last_history(self.history_path)\n",
    "        \n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = self.env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # obs_batch = torch.tensor(np.array(state), device=self.device, dtype=torch.float32)\n",
    "                state = state.unsqueeze(0)\n",
    "                q_values = self.model(state, 1).to(self.device)\n",
    "                action = torch.argmax(q_values).item() % self.batch_size\n",
    "                # self.log = f'{q_values}, {action}'\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        # Sample a minibatch from replay memory\n",
    "        batch = self.replay_buffer.sample(self.batch_size)\n",
    "        batch = self.Experience(*zip(*batch))\n",
    "\n",
    "        action_batch = torch.tensor(batch[1], device=self.device, dtype=torch.long)\n",
    "        reward_batch = torch.tensor(batch[2], device=self.device, dtype=torch.float)\n",
    "        done_batch = torch.tensor(batch[4], device=self.device, dtype=torch.float)\n",
    "\n",
    "        # state_batch = torch.tensor(batch[0], device=self.device, dtype=torch.float32)\n",
    "        # next_state_batch = torch.tensor(batch[3], device=self.device, dtype=torch.float32)\n",
    "        state_batch = torch.stack(batch[0])\n",
    "        next_state_batch = torch.stack(batch[3])\n",
    "\n",
    "        # Calculate the Q-values for the current state\n",
    "        q_values = self.model(state_batch).to(self.device)\n",
    "        q_values = q_values.gather(1, action_batch.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Calculate the target Q-values using the target network\n",
    "        with torch.no_grad():\n",
    "            target_q_values = self.target_model(next_state_batch).to(self.device)\n",
    "            target_q_values = target_q_values.gather(1, action_batch.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            target_q_values = reward_batch + self.gamma * (1 - done_batch) * target_q_values\n",
    "\n",
    "        # Calculate the loss and perform gradient descent\n",
    "        loss = F.smooth_l1_loss(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        history_data = []\n",
    "        save_after = 10\n",
    "        for episode in range(num_episodes):\n",
    "            obs = transform(self.wrapped_env.reset()[0], False)\n",
    "\n",
    "            # plt.imshow(obs)\n",
    "            # plt.show()\n",
    "            \n",
    "            # Render the environment\n",
    "            # env.render()\n",
    "\n",
    "            done = False\n",
    "            total_trad_reward = 0\n",
    "            total_nontrad_reward = 0\n",
    "            timestep = 0\n",
    "\n",
    "            while not done:\n",
    "                # Epsilon-greedy action selection\n",
    "                # obs = (np.rint(obs)).astype(int)\n",
    "                action = self.act(obs)\n",
    "\n",
    "                # Take action and observe next state and reward\n",
    "                next_obs, reward, done, err, info = self.wrapped_env.step(action)\n",
    "                \n",
    "                # Render the environment\n",
    "                # self.env.render()\n",
    "                \n",
    "                # get traditional and non-traditional reward\n",
    "                trad, nontrad = compute_reward(reward, done)\n",
    "                \n",
    "                combined_reward = trad + nontrad\n",
    "                total_trad_reward += trad\n",
    "                total_nontrad_reward += nontrad\n",
    "                \n",
    "                # next_obs = (np.rint(next_obs)).astype(int)\n",
    "                \n",
    "                # Store the experience in the replay buffer\n",
    "                next_obs = transform(next_obs, False)\n",
    "                experience = self.Experience(obs, action, combined_reward, next_obs, done)\n",
    "                self.replay_buffer.push(experience)\n",
    "\n",
    "                # Update the current state\n",
    "                obs = next_obs\n",
    "                \n",
    "                # model learning\n",
    "                self.learn()\n",
    "                timestep += 1\n",
    "                \n",
    "            # Update the target network\n",
    "            if episode % self.target_update_freq == 0:\n",
    "                self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "            # Decay epsilon\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            total_nontrad_reward = decimal2(total_nontrad_reward)\n",
    "            total_reward = total_trad_reward + total_nontrad_reward\n",
    "            \n",
    "            # Print the total reward for the episode\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}, Traditional: {total_trad_reward}\")\n",
    "            \n",
    "            # persist history and policy after every 20 episodes\n",
    "            history_data.append(f\"{int(self.current_episode) + episode + 1}, {total_reward}, {total_trad_reward}, {total_nontrad_reward}, {timestep}, {self.log}\")\n",
    "            if ((episode + 1) % save_after == 0):\n",
    "                # Save the experiencel\n",
    "                torch.save(self.model.state_dict(), self.policy_path)\n",
    "                \n",
    "                # write history\n",
    "                write_history(self.history_path, history_data)\n",
    "                history_data = []\n",
    "\n",
    "    def evaluate(self, num_episodes):\n",
    "        wins = 0\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()[0]\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _, __ = self.env.step(action)\n",
    "                state = next_state\n",
    "                if reward > 0:\n",
    "                    wins += 1\n",
    "        print(f'Wins - {wins}; Episodes - {num_episodes}; Average - {wins / num_episodes}') \n",
    "    \n",
    "    def close_session(self):\n",
    "        self.wrapped_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: 16.29, Traditional: 0\n",
      "Episode: 1, Total Reward: 24.8, Traditional: 1\n",
      "Episode: 2, Total Reward: 22.2, Traditional: 1\n",
      "Episode: 3, Total Reward: 20.8, Traditional: 1\n",
      "Episode: 4, Total Reward: 23.2, Traditional: 1\n",
      "Episode: 5, Total Reward: 28.9, Traditional: 2\n",
      "Episode: 6, Total Reward: 28.8, Traditional: 2\n",
      "Episode: 7, Total Reward: 22.5, Traditional: 1\n",
      "Episode: 8, Total Reward: 22.0, Traditional: 1\n",
      "Episode: 9, Total Reward: 17.19, Traditional: 0\n",
      "---- starting environment 0 ----\n",
      "Episode: 0, Total Reward: 24.2, Traditional: 1\n",
      "Episode: 1, Total Reward: 21.7, Traditional: 1\n",
      "Episode: 2, Total Reward: 28.5, Traditional: 1\n",
      "Episode: 3, Total Reward: 44.1, Traditional: 4\n",
      "Episode: 4, Total Reward: 31.6, Traditional: 2\n",
      "Episode: 5, Total Reward: 31.9, Traditional: 2\n",
      "Episode: 6, Total Reward: 17.39, Traditional: 0\n",
      "Episode: 7, Total Reward: 25.1, Traditional: 1\n",
      "Episode: 8, Total Reward: 23.5, Traditional: 1\n",
      "Episode: 9, Total Reward: 32.5, Traditional: 2\n",
      "---- starting environment 1 ----\n",
      "Episode: 0, Total Reward: 17.69, Traditional: 0\n",
      "Episode: 1, Total Reward: 18.79, Traditional: 0\n",
      "Episode: 2, Total Reward: 27.9, Traditional: 2\n",
      "Episode: 3, Total Reward: 17.09, Traditional: 0\n",
      "Episode: 4, Total Reward: 18.49, Traditional: 0\n",
      "Episode: 5, Total Reward: 18.79, Traditional: 0\n",
      "Episode: 6, Total Reward: 41.8, Traditional: 4\n",
      "Episode: 7, Total Reward: 31.8, Traditional: 2\n",
      "Episode: 8, Total Reward: 24.5, Traditional: 1\n",
      "Episode: 9, Total Reward: 17.99, Traditional: 0\n",
      "---- starting environment 2 ----\n",
      "Episode: 0, Total Reward: 36.7, Traditional: 3\n",
      "Episode: 1, Total Reward: 16.49, Traditional: 0\n",
      "Episode: 2, Total Reward: 24.7, Traditional: 1\n",
      "Episode: 3, Total Reward: 32.1, Traditional: 2\n",
      "Episode: 4, Total Reward: 30.8, Traditional: 2\n",
      "Episode: 5, Total Reward: 22.0, Traditional: 1\n",
      "Episode: 6, Total Reward: 39.6, Traditional: 4\n",
      "Episode: 7, Total Reward: 33.6, Traditional: 2\n",
      "Episode: 8, Total Reward: 23.8, Traditional: 1\n",
      "Episode: 9, Total Reward: 24.1, Traditional: 1\n",
      "---- starting environment 3 ----\n",
      "Episode: 0, Total Reward: 24.6, Traditional: 1\n",
      "Episode: 1, Total Reward: 27.9, Traditional: 1\n",
      "Episode: 2, Total Reward: 22.2, Traditional: 1\n",
      "Episode: 3, Total Reward: 40.1, Traditional: 4\n",
      "Episode: 4, Total Reward: 41.3, Traditional: 4\n",
      "Episode: 5, Total Reward: 29.6, Traditional: 2\n",
      "Episode: 6, Total Reward: 22.0, Traditional: 1\n",
      "Episode: 7, Total Reward: 38.8, Traditional: 3\n",
      "Episode: 8, Total Reward: 16.89, Traditional: 0\n",
      "Episode: 9, Total Reward: 22.8, Traditional: 1\n",
      "---- starting environment 4 ----\n",
      "Episode: 0, Total Reward: 39.8, Traditional: 3\n",
      "Episode: 1, Total Reward: 29.6, Traditional: 2\n",
      "Episode: 2, Total Reward: 17.39, Traditional: 0\n",
      "Episode: 3, Total Reward: 32.3, Traditional: 2\n",
      "Episode: 4, Total Reward: 29.1, Traditional: 2\n",
      "Episode: 5, Total Reward: 16.69, Traditional: 0\n",
      "Episode: 6, Total Reward: 32.0, Traditional: 2\n",
      "Episode: 7, Total Reward: 36.4, Traditional: 3\n",
      "Episode: 8, Total Reward: 17.49, Traditional: 0\n",
      "Episode: 9, Total Reward: 29.2, Traditional: 2\n",
      "---- starting environment 5 ----\n",
      "Episode: 0, Total Reward: 25.2, Traditional: 1\n",
      "Episode: 1, Total Reward: 25.2, Traditional: 1\n",
      "Episode: 2, Total Reward: 18.09, Traditional: 0\n",
      "Episode: 3, Total Reward: 48.9, Traditional: 5\n",
      "Episode: 4, Total Reward: 21.9, Traditional: 1\n",
      "Episode: 5, Total Reward: 29.7, Traditional: 2\n",
      "Episode: 6, Total Reward: 18.09, Traditional: 0\n",
      "Episode: 7, Total Reward: 17.69, Traditional: 0\n",
      "Episode: 8, Total Reward: 22.4, Traditional: 1\n",
      "Episode: 9, Total Reward: 24.1, Traditional: 1\n",
      "---- starting environment 6 ----\n",
      "Episode: 0, Total Reward: 24.2, Traditional: 1\n",
      "Episode: 1, Total Reward: 26.7, Traditional: 1\n",
      "Episode: 2, Total Reward: 30.7, Traditional: 2\n",
      "Episode: 3, Total Reward: 24.3, Traditional: 1\n",
      "Episode: 4, Total Reward: 16.29, Traditional: 0\n",
      "Episode: 5, Total Reward: 44.7, Traditional: 4\n",
      "Episode: 6, Total Reward: 32.2, Traditional: 2\n",
      "Episode: 7, Total Reward: 31.4, Traditional: 2\n",
      "Episode: 8, Total Reward: 26.2, Traditional: 2\n",
      "Episode: 9, Total Reward: 22.6, Traditional: 1\n",
      "---- starting environment 7 ----\n",
      "Episode: 0, Total Reward: 25.8, Traditional: 1\n",
      "Episode: 1, Total Reward: 30.1, Traditional: 2\n",
      "Episode: 2, Total Reward: 41.4, Traditional: 4\n",
      "Episode: 3, Total Reward: 21.3, Traditional: 1\n",
      "Episode: 4, Total Reward: 43.7, Traditional: 4\n",
      "Episode: 5, Total Reward: 17.79, Traditional: 0\n",
      "Episode: 6, Total Reward: 16.29, Traditional: 0\n",
      "Episode: 7, Total Reward: 29.2, Traditional: 2\n",
      "Episode: 8, Total Reward: 23.8, Traditional: 1\n",
      "Episode: 9, Total Reward: 17.29, Traditional: 0\n",
      "---- starting environment 8 ----\n",
      "Episode: 0, Total Reward: 39.2, Traditional: 4\n",
      "Episode: 1, Total Reward: 22.8, Traditional: 1\n",
      "Episode: 2, Total Reward: 51.2, Traditional: 5\n",
      "Episode: 3, Total Reward: 26.2, Traditional: 1\n",
      "Episode: 4, Total Reward: 16.89, Traditional: 0\n",
      "Episode: 5, Total Reward: 17.99, Traditional: 0\n",
      "Episode: 6, Total Reward: 19.0, Traditional: 0\n",
      "Episode: 7, Total Reward: 25.3, Traditional: 1\n",
      "Episode: 8, Total Reward: 34.1, Traditional: 3\n",
      "Episode: 9, Total Reward: 17.89, Traditional: 0\n",
      "---- starting environment 9 ----\n",
      "Episode: 0, Total Reward: 30.1, Traditional: 2\n",
      "Episode: 1, Total Reward: 25.1, Traditional: 1\n",
      "Episode: 2, Total Reward: 17.69, Traditional: 0\n",
      "Episode: 3, Total Reward: 17.69, Traditional: 0\n",
      "Episode: 4, Total Reward: 17.99, Traditional: 0\n",
      "Episode: 5, Total Reward: 19.4, Traditional: 0\n",
      "Episode: 6, Total Reward: 19.4, Traditional: 0\n",
      "Episode: 7, Total Reward: 28.4, Traditional: 2\n",
      "Episode: 8, Total Reward: 26.9, Traditional: 2\n",
      "Episode: 9, Total Reward: 16.79, Traditional: 0\n",
      "---- starting environment 10 ----\n",
      "Episode: 0, Total Reward: 29.1, Traditional: 2\n",
      "Episode: 1, Total Reward: 17.59, Traditional: 0\n",
      "Episode: 2, Total Reward: 18.39, Traditional: 0\n",
      "Episode: 3, Total Reward: 22.3, Traditional: 1\n",
      "Episode: 4, Total Reward: 18.19, Traditional: 0\n",
      "Episode: 5, Total Reward: 27.0, Traditional: 2\n",
      "Episode: 6, Total Reward: 23.3, Traditional: 1\n",
      "Episode: 7, Total Reward: 17.59, Traditional: 0\n",
      "Episode: 8, Total Reward: 31.7, Traditional: 2\n",
      "Episode: 9, Total Reward: 24.4, Traditional: 1\n",
      "---- starting environment 11 ----\n",
      "Episode: 0, Total Reward: 22.7, Traditional: 1\n",
      "Episode: 1, Total Reward: 17.09, Traditional: 0\n",
      "Episode: 2, Total Reward: 22.4, Traditional: 1\n",
      "Episode: 3, Total Reward: 16.19, Traditional: 0\n",
      "Episode: 4, Total Reward: 17.79, Traditional: 0\n",
      "Episode: 5, Total Reward: 21.1, Traditional: 1\n",
      "Episode: 6, Total Reward: 18.19, Traditional: 0\n",
      "Episode: 7, Total Reward: 23.8, Traditional: 1\n",
      "Episode: 8, Total Reward: 26.5, Traditional: 2\n",
      "Episode: 9, Total Reward: 25.2, Traditional: 1\n",
      "---- starting environment 12 ----\n",
      "Episode: 0, Total Reward: 34.7, Traditional: 3\n",
      "Episode: 1, Total Reward: 16.79, Traditional: 0\n",
      "Episode: 2, Total Reward: 18.39, Traditional: 0\n",
      "Episode: 3, Total Reward: 33.8, Traditional: 2\n",
      "Episode: 4, Total Reward: 16.59, Traditional: 0\n",
      "Episode: 5, Total Reward: 27.0, Traditional: 2\n",
      "Episode: 6, Total Reward: 17.49, Traditional: 0\n",
      "Episode: 7, Total Reward: 17.29, Traditional: 0\n",
      "Episode: 8, Total Reward: 18.09, Traditional: 0\n",
      "Episode: 9, Total Reward: 24.6, Traditional: 1\n",
      "---- starting environment 13 ----\n",
      "Episode: 0, Total Reward: 16.99, Traditional: 0\n",
      "Episode: 1, Total Reward: 28.1, Traditional: 2\n",
      "Episode: 2, Total Reward: 32.7, Traditional: 2\n",
      "Episode: 3, Total Reward: 17.19, Traditional: 0\n",
      "Episode: 4, Total Reward: 52.6, Traditional: 5\n",
      "Episode: 5, Total Reward: 24.9, Traditional: 1\n",
      "Episode: 6, Total Reward: 28.7, Traditional: 2\n",
      "Episode: 7, Total Reward: 24.8, Traditional: 1\n",
      "Episode: 8, Total Reward: 23.1, Traditional: 1\n",
      "Episode: 9, Total Reward: 17.09, Traditional: 0\n",
      "---- starting environment 14 ----\n",
      "Episode: 0, Total Reward: 18.39, Traditional: 0\n",
      "Episode: 1, Total Reward: 19.9, Traditional: 0\n",
      "Episode: 2, Total Reward: 22.6, Traditional: 1\n",
      "Episode: 3, Total Reward: 24.4, Traditional: 1\n",
      "Episode: 4, Total Reward: 31.4, Traditional: 2\n",
      "Episode: 5, Total Reward: 20.8, Traditional: 1\n",
      "Episode: 6, Total Reward: 18.59, Traditional: 0\n",
      "Episode: 7, Total Reward: 40.7, Traditional: 3\n",
      "Episode: 8, Total Reward: 17.59, Traditional: 0\n",
      "Episode: 9, Total Reward: 17.59, Traditional: 0\n",
      "---- starting environment 15 ----\n",
      "Episode: 0, Total Reward: 29.9, Traditional: 2\n",
      "Episode: 1, Total Reward: 31.9, Traditional: 2\n",
      "Episode: 2, Total Reward: 17.79, Traditional: 0\n",
      "Episode: 3, Total Reward: 32.6, Traditional: 2\n",
      "Episode: 4, Total Reward: 32.5, Traditional: 3\n",
      "Episode: 5, Total Reward: 31.6, Traditional: 2\n",
      "Episode: 6, Total Reward: 24.3, Traditional: 1\n",
      "Episode: 7, Total Reward: 17.69, Traditional: 0\n",
      "Episode: 8, Total Reward: 29.2, Traditional: 2\n",
      "Episode: 9, Total Reward: 17.49, Traditional: 0\n",
      "---- starting environment 16 ----\n",
      "Episode: 0, Total Reward: 17.79, Traditional: 0\n",
      "Episode: 1, Total Reward: 29.8, Traditional: 2\n",
      "Episode: 2, Total Reward: 25.5, Traditional: 1\n",
      "Episode: 3, Total Reward: 29.8, Traditional: 2\n",
      "Episode: 4, Total Reward: 41.6, Traditional: 3\n",
      "Episode: 5, Total Reward: 21.3, Traditional: 0\n",
      "Episode: 6, Total Reward: 18.29, Traditional: 0\n",
      "Episode: 7, Total Reward: 16.89, Traditional: 0\n",
      "Episode: 8, Total Reward: 37.4, Traditional: 3\n",
      "Episode: 9, Total Reward: 36.6, Traditional: 3\n",
      "---- starting environment 17 ----\n",
      "Episode: 0, Total Reward: 17.29, Traditional: 0\n",
      "Episode: 1, Total Reward: 16.99, Traditional: 0\n",
      "Episode: 2, Total Reward: 22.7, Traditional: 1\n",
      "Episode: 3, Total Reward: 28.1, Traditional: 2\n",
      "Episode: 4, Total Reward: 54.8, Traditional: 6\n",
      "Episode: 5, Total Reward: 22.7, Traditional: 1\n",
      "Episode: 6, Total Reward: 16.69, Traditional: 0\n",
      "Episode: 7, Total Reward: 17.49, Traditional: 0\n",
      "Episode: 8, Total Reward: 16.79, Traditional: 0\n",
      "Episode: 9, Total Reward: 36.8, Traditional: 3\n",
      "---- starting environment 18 ----\n",
      "Episode: 0, Total Reward: 25.7, Traditional: 1\n",
      "Episode: 1, Total Reward: 23.8, Traditional: 1\n",
      "Episode: 2, Total Reward: 16.49, Traditional: 0\n",
      "Episode: 3, Total Reward: 23.5, Traditional: 1\n",
      "Episode: 4, Total Reward: 36.9, Traditional: 3\n",
      "Episode: 5, Total Reward: 17.69, Traditional: 0\n",
      "Episode: 6, Total Reward: 24.6, Traditional: 1\n",
      "Episode: 7, Total Reward: 22.1, Traditional: 1\n",
      "Episode: 8, Total Reward: 25.3, Traditional: 1\n",
      "Episode: 9, Total Reward: 28.8, Traditional: 2\n",
      "---- starting environment 19 ----\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "  env = gym.make(\"Breakout-v4\")\n",
    "  env_name = env.spec.id.split('-')[0]\n",
    "  agent = DQNAgent(env, env_name)\n",
    "  agent.train(num_episodes=10)\n",
    "  agent.close_session()\n",
    "\n",
    "  del env\n",
    "  del agent\n",
    "  \n",
    "  print(f'---- starting environment {i} ----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins - 22; Episodes - 20; Average - 1.1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Breakout-v4\")\n",
    "env_name = env.spec.id.split('-')[0]\n",
    "agent = DQNAgent(env, env_name)\n",
    "episodes = 20\n",
    "agent.evaluate(episodes)\n",
    "agent.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import torch.nn.functional as F\n",
    "from skimage.transform import resize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import ReplayBuffer, label_with_episode_number, save_random_agent_gif, file_exists, write_history, get_last_history, compute_reward, decimal2\n",
    "\n",
    "from fileIO import FileIO\n",
    "\n",
    "class SARSAAgent(object):\n",
    "    def __init__(self, env, env_name, eval_mode = False):\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"mps\")\n",
    "        self.name = env_name.lower()\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.state_size = (210, 160, 3)\n",
    "        # Flatten the state shape\n",
    "        state_space = int(env.observation_space.high.sum())\n",
    "        \n",
    "        # Define hyperparameters\n",
    "        self.learning_rate = 0.1\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.epsilon_min = 0.01\n",
    "        self.target_update_freq = 10\n",
    "\n",
    "        self.resume = False\n",
    "        self.eval_mode = eval_mode\n",
    "        self.policy_path = f'saves/{self.name}/sarsa_policy.npy'\n",
    "        self.history_path = f'saves/{self.name}/sarsa_history.csv'\n",
    "        self.log = ''\n",
    "        \n",
    "        # Define the replay buffer\n",
    "        self.Experience = np.zeros((state_space, int(self.action_dim)))\n",
    "\n",
    "        # save time by retraining model from a known trained weights\n",
    "        if (file_exists(self.policy_path)):\n",
    "            self.resume = True\n",
    "            self.Experience = np.load(self.policy_path)\n",
    "            print('----- loaded saved weights ------')\n",
    "            \n",
    "\n",
    "        # implement evaluation history so traning can be ran in batches to not crash system\n",
    "        self.current_episode =  get_last_history(self.history_path)\n",
    "        \n",
    "    def state_index(self, obs):\n",
    "        obs_index = obs.sum()\n",
    "        \n",
    "        return int(obs_index)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon or not self.eval_mode:\n",
    "            action = self.env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            action = np.argmax(self.Experience[self.state_index(state)])  # Exploit\n",
    "\n",
    "        return action\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        history_data = []\n",
    "        save_after = 100\n",
    "        for episode in range(num_episodes):\n",
    "            obs = self.env.reset()[0]\n",
    "            action = self.act(obs)\n",
    "\n",
    "            # plt.imshow(obs)\n",
    "            # plt.show()\n",
    "\n",
    "            done = False\n",
    "            total_trad_reward = 0\n",
    "            total_nontrad_reward = 0\n",
    "            timestep = 0\n",
    "\n",
    "            while not done:\n",
    "                # Take action and observe next state and reward\n",
    "                next_obs, reward, done, err, info = self.env.step(action)\n",
    "                next_action = self.act(next_obs)\n",
    "                \n",
    "                # Render the environment\n",
    "                # self.env.render()\n",
    "                \n",
    "                # get traditional and non-traditional reward\n",
    "                trad, nontrad = compute_reward(reward, done)\n",
    "                \n",
    "                combined_reward = trad + nontrad\n",
    "                total_trad_reward += trad\n",
    "                total_nontrad_reward += nontrad\n",
    "                \n",
    "                # Update Q-table\n",
    "                q_value = self.Experience[self.state_index(obs), action]\n",
    "                next_q_value = self.Experience[self.state_index(next_obs), next_action]\n",
    "                td_error = combined_reward + self.gamma * next_q_value - q_value\n",
    "                self.Experience[self.state_index(obs), action] += self.learning_rate * td_error\n",
    "\n",
    "                # Update the current state\n",
    "                obs = next_obs\n",
    "                action = next_action\n",
    "                timestep += 1\n",
    "                \n",
    "            total_nontrad_reward = decimal2(total_nontrad_reward)\n",
    "            total_reward = total_trad_reward + total_nontrad_reward\n",
    "\n",
    "            # Print the total reward for the episode\n",
    "            print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Traditional: {total_trad_reward}\")\n",
    "            \n",
    "            # persist history and policy after every 20 episodes\n",
    "            history_data.append(f\"{self.current_episode + episode + 1}, {total_reward}, {total_trad_reward}, {total_nontrad_reward}, {timestep}, {action}\")\n",
    "            if ((episode + 1) % save_after == 0):\n",
    "                # Save the experience\n",
    "                np.save(self.policy_path, self.Experience)\n",
    "                \n",
    "                # write history\n",
    "                write_history(self.history_path, history_data)\n",
    "                history_data = []\n",
    "\n",
    "    def evaluate(self, num_episodes):\n",
    "        wins = 0\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()[0]\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _, __ = self.env.step(action)\n",
    "                state = next_state\n",
    "                if reward > 0:\n",
    "                    wins += 1\n",
    "        print(f'Wins - {wins}; Episodes - {num_episodes}; Average - {wins / num_episodes}') \n",
    "    \n",
    "    def close_session(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_name = 'Breakout-v4'\n",
    "env_name = 'Berzerk-v4'\n",
    "training_episodes = 100\n",
    "eval_episodes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- loaded saved weights ------\n",
      "Episode: 1, Total Reward: 23.6, Traditional: 1\n",
      "Episode: 2, Total Reward: 157.09, Traditional: 7\n",
      "Episode: 3, Total Reward: 28.1, Traditional: 3\n",
      "Episode: 4, Total Reward: 20.7, Traditional: 0\n",
      "Episode: 5, Total Reward: 27.1, Traditional: 3\n",
      "Episode: 6, Total Reward: 29.7, Traditional: 2\n",
      "Episode: 7, Total Reward: 19.79, Traditional: 2\n",
      "Episode: 8, Total Reward: 25.5, Traditional: 2\n",
      "Episode: 9, Total Reward: 32.3, Traditional: 3\n",
      "Episode: 10, Total Reward: 18.39, Traditional: 0\n",
      "Episode: 11, Total Reward: 34.7, Traditional: 3\n",
      "Episode: 12, Total Reward: 21.2, Traditional: 1\n",
      "Episode: 13, Total Reward: 36.8, Traditional: 2\n",
      "Episode: 14, Total Reward: 29.1, Traditional: 2\n",
      "Episode: 15, Total Reward: 29.6, Traditional: 3\n",
      "Episode: 16, Total Reward: 39.6, Traditional: 4\n",
      "Episode: 17, Total Reward: 54.0, Traditional: 9\n",
      "Episode: 18, Total Reward: 23.2, Traditional: 0\n",
      "Episode: 19, Total Reward: 29.3, Traditional: 4\n",
      "Episode: 20, Total Reward: 22.8, Traditional: 1\n",
      "Episode: 21, Total Reward: 21.9, Traditional: 1\n",
      "Episode: 22, Total Reward: 24.0, Traditional: 3\n",
      "Episode: 23, Total Reward: 119.29, Traditional: 11\n",
      "Episode: 24, Total Reward: 50.5, Traditional: 4\n",
      "Episode: 25, Total Reward: 18.49, Traditional: 0\n",
      "Episode: 26, Total Reward: 31.6, Traditional: 7\n",
      "Episode: 27, Total Reward: 27.7, Traditional: 3\n",
      "Episode: 28, Total Reward: 86.69, Traditional: 7\n",
      "Episode: 29, Total Reward: 39.4, Traditional: 3\n",
      "Episode: 30, Total Reward: 29.0, Traditional: 3\n",
      "Episode: 31, Total Reward: 21.3, Traditional: 0\n",
      "Episode: 32, Total Reward: 21.9, Traditional: 0\n",
      "Episode: 33, Total Reward: 24.7, Traditional: 2\n",
      "Episode: 34, Total Reward: 21.3, Traditional: 1\n",
      "Episode: 35, Total Reward: 31.9, Traditional: 3\n",
      "Episode: 36, Total Reward: 25.9, Traditional: 1\n",
      "Episode: 37, Total Reward: 30.4, Traditional: 2\n",
      "Episode: 38, Total Reward: 82.8, Traditional: 9\n",
      "Episode: 39, Total Reward: 20.2, Traditional: 1\n",
      "Episode: 40, Total Reward: 38.0, Traditional: 4\n",
      "Episode: 41, Total Reward: 36.2, Traditional: 2\n",
      "Episode: 42, Total Reward: 22.5, Traditional: 3\n",
      "Episode: 43, Total Reward: 23.4, Traditional: 2\n",
      "Episode: 44, Total Reward: 19.39, Traditional: 1\n",
      "Episode: 45, Total Reward: 65.8, Traditional: 5\n",
      "Episode: 46, Total Reward: 26.8, Traditional: 3\n",
      "Episode: 47, Total Reward: 22.6, Traditional: 1\n",
      "Episode: 48, Total Reward: 45.0, Traditional: 2\n",
      "Episode: 49, Total Reward: 78.9, Traditional: 8\n",
      "Episode: 50, Total Reward: 31.2, Traditional: 5\n",
      "Episode: 51, Total Reward: 30.0, Traditional: 4\n",
      "Episode: 52, Total Reward: 27.1, Traditional: 1\n",
      "Episode: 53, Total Reward: 23.5, Traditional: 0\n",
      "Episode: 54, Total Reward: 23.7, Traditional: 1\n",
      "Episode: 55, Total Reward: 38.4, Traditional: 5\n",
      "Episode: 56, Total Reward: 43.7, Traditional: 5\n",
      "Episode: 57, Total Reward: 19.8, Traditional: 0\n",
      "Episode: 58, Total Reward: 65.7, Traditional: 5\n",
      "Episode: 59, Total Reward: 25.0, Traditional: 2\n",
      "Episode: 60, Total Reward: 37.0, Traditional: 1\n",
      "Episode: 61, Total Reward: 42.5, Traditional: 4\n",
      "Episode: 62, Total Reward: 25.6, Traditional: 1\n",
      "Episode: 63, Total Reward: 33.1, Traditional: 3\n",
      "Episode: 64, Total Reward: 19.8, Traditional: 0\n",
      "Episode: 65, Total Reward: 21.0, Traditional: 2\n",
      "Episode: 66, Total Reward: 20.6, Traditional: 1\n",
      "Episode: 67, Total Reward: 19.39, Traditional: 1\n",
      "Episode: 68, Total Reward: 31.4, Traditional: 3\n",
      "Episode: 69, Total Reward: 32.1, Traditional: 2\n",
      "Episode: 70, Total Reward: 43.0, Traditional: 2\n",
      "Episode: 71, Total Reward: 21.6, Traditional: 1\n",
      "Episode: 72, Total Reward: 19.6, Traditional: 0\n",
      "Episode: 73, Total Reward: 55.6, Traditional: 5\n",
      "Episode: 74, Total Reward: 26.2, Traditional: 1\n",
      "Episode: 75, Total Reward: 32.8, Traditional: 2\n",
      "Episode: 76, Total Reward: 40.1, Traditional: 4\n",
      "Episode: 77, Total Reward: 24.9, Traditional: 3\n",
      "Episode: 78, Total Reward: 65.0, Traditional: 6\n",
      "Episode: 79, Total Reward: 55.4, Traditional: 6\n",
      "Episode: 80, Total Reward: 19.2, Traditional: 0\n",
      "Episode: 81, Total Reward: 28.7, Traditional: 3\n",
      "Episode: 82, Total Reward: 53.3, Traditional: 7\n",
      "Episode: 83, Total Reward: 44.9, Traditional: 5\n",
      "Episode: 84, Total Reward: 51.1, Traditional: 7\n",
      "Episode: 85, Total Reward: 18.29, Traditional: 0\n",
      "Episode: 86, Total Reward: 20.9, Traditional: 1\n",
      "Episode: 87, Total Reward: 26.3, Traditional: 3\n",
      "Episode: 88, Total Reward: 35.3, Traditional: 3\n",
      "Episode: 89, Total Reward: 208.09, Traditional: 7\n",
      "Episode: 90, Total Reward: 35.5, Traditional: 5\n",
      "Episode: 91, Total Reward: 21.8, Traditional: 1\n",
      "Episode: 92, Total Reward: 29.3, Traditional: 4\n",
      "Episode: 93, Total Reward: 41.2, Traditional: 3\n",
      "Episode: 94, Total Reward: 55.6, Traditional: 6\n",
      "Episode: 95, Total Reward: 31.2, Traditional: 3\n",
      "Episode: 96, Total Reward: 52.9, Traditional: 5\n",
      "Episode: 97, Total Reward: 36.2, Traditional: 6\n",
      "Episode: 98, Total Reward: 30.4, Traditional: 4\n",
      "Episode: 99, Total Reward: 25.3, Traditional: 3\n",
      "Episode: 100, Total Reward: 43.6, Traditional: 3\n",
      "---- end environment ----\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "env_name = env.spec.id.split('-')[0]\n",
    "agent = SARSAAgent(env, env_name)\n",
    "agent.train(num_episodes=training_episodes)\n",
    "\n",
    "agent.close_session()\n",
    "\n",
    "print(f'---- end environment ----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/korede/miniconda3/lib/python3.10/site-packages/gym/envs/registration.py:563: UserWarning: \u001b[33mWARN: Using the latest versioned environment `Berzerk-v4` instead of the unversioned environment `Berzerk`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- loaded saved weights ------\n",
      "Wins - 42; Episodes - 10; Average - 4.2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "env_name = env.spec.id.split('-')[0]\n",
    "agent = SARSAAgent(env, env_name, True)\n",
    "agent.evaluate(eval_episodes)\n",
    "agent.close_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

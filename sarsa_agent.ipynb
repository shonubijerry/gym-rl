{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import torch.nn.functional as F\n",
    "from skimage.transform import resize\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import ReplayBuffer, label_with_episode_number, save_random_agent_gif, file_exists, write_history, get_last_history, compute_reward, decimal2\n",
    "\n",
    "from environment_wrapper import EnvironmentWrapper, ObservationWrapper, RewardWrapper, ActionWrapper\n",
    "from fileIO import FileIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SARSAAgent(object):\n",
    "    def __init__(self, env, env_name):\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"mps\")\n",
    "        self.name = env_name.lower()\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.state_size = (210, 160, 3)\n",
    "        # Flatten the state shape\n",
    "        state_space = int(env.observation_space.high.sum())\n",
    "        \n",
    "        # Define hyperparameters\n",
    "        self.learning_rate = 0.1\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.epsilon_min = 0.01\n",
    "        self.target_update_freq = 10\n",
    "\n",
    "        self.resume = False\n",
    "        self.policy_path = f'saves/{self.name}/sarsa_policy.npy'\n",
    "        self.history_path = f'saves/{self.name}/sarsa_history.csv'\n",
    "        self.log = ''\n",
    "        \n",
    "        # Define the replay buffer\n",
    "        self.Experience = np.zeros((state_space, int(self.action_dim)))\n",
    "\n",
    "        # save time by retraining model from a known trained weights\n",
    "        if (file_exists(self.policy_path)):\n",
    "            self.resume = True\n",
    "            self.Experience = np.load(self.policy_path)\n",
    "            \n",
    "\n",
    "        # implement evaluation history so traning can be ran in batches to not crash system\n",
    "        self.current_episode =  get_last_history(self.history_path)\n",
    "        \n",
    "    def state_index(self, obs):\n",
    "        obs_index = obs.sum()\n",
    "        \n",
    "        return int(obs_index)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = self.env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            action = np.argmax(self.Experience[self.state_index(state)])  # Exploit\n",
    "\n",
    "        return action\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        history_data = []\n",
    "        save_after = 50\n",
    "        for episode in range(num_episodes):\n",
    "            obs = self.env.reset()[0]\n",
    "            action = self.act(obs)\n",
    "\n",
    "            # plt.imshow(obs)\n",
    "            # plt.show()\n",
    "\n",
    "            done = False\n",
    "            total_trad_reward = 0\n",
    "            total_nontrad_reward = 0\n",
    "            timestep = 0\n",
    "\n",
    "            while not done:\n",
    "                # Take action and observe next state and reward\n",
    "                next_obs, reward, done, err, info = self.env.step(action)\n",
    "                next_action = self.act(next_obs)\n",
    "                \n",
    "                # Render the environment\n",
    "                # self.env.render()\n",
    "                \n",
    "                # get traditional and non-traditional reward\n",
    "                trad, nontrad = compute_reward(reward, done)\n",
    "                \n",
    "                combined_reward = trad + nontrad\n",
    "                total_trad_reward += trad\n",
    "                total_nontrad_reward += nontrad\n",
    "                \n",
    "                # Update Q-table\n",
    "                q_value = self.Experience[self.state_index(obs), action]\n",
    "                next_q_value = self.Experience[self.state_index(next_obs), next_action]\n",
    "                td_error = combined_reward + self.gamma * next_q_value - q_value\n",
    "                self.Experience[self.state_index(obs), action] += self.learning_rate * td_error\n",
    "\n",
    "                # Update the current state\n",
    "                obs = next_obs\n",
    "                action = next_action\n",
    "                timestep += 1\n",
    "                \n",
    "            total_nontrad_reward = decimal2(total_nontrad_reward)\n",
    "            total_reward = total_trad_reward + total_nontrad_reward\n",
    "\n",
    "            # Print the total reward for the episode\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
    "            \n",
    "            # persist history and policy after every 20 episodes\n",
    "            history_data.append(f\"{self.current_episode + episode + 1}, {total_reward}, {total_trad_reward}, {total_nontrad_reward}, {timestep}, {action}\")\n",
    "            if ((episode + 1) % save_after == 0):\n",
    "                # Save the experience\n",
    "                np.save(self.policy_path, self.Experience)\n",
    "                \n",
    "                # write history\n",
    "                write_history(self.history_path, history_data)\n",
    "                history_data = []\n",
    "\n",
    "    def evaluate(self, num_episodes):\n",
    "        wins = 0\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()[0]\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _, __ = self.env.step(action)\n",
    "                state = next_state\n",
    "                if reward > 0:\n",
    "                    wins += 1\n",
    "        print(f'Wins - {wins}; Episodes - {num_episodes}; Average - {wins / num_episodes}') \n",
    "    \n",
    "    def close_session(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Breakout-v4\")\n",
    "env_name = env.spec.id.split('-')[0]\n",
    "agent = SARSAAgent(env, env_name)\n",
    "agent.train(num_episodes=200)\n",
    "\n",
    "agent.close_session()\n",
    "\n",
    "print(f'---- end environment ----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins - 30; Episodes - 20; Average - 1.5\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Breakout-v4\")\n",
    "env_name = env.spec.id.split('-')[0]\n",
    "agent = SARSAAgent(env, env_name)\n",
    "episodes = 20\n",
    "agent.evaluate(episodes)\n",
    "agent.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

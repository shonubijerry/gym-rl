{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/korede/miniconda3/lib/python3.10/site-packages/gym/envs/toy_text/frozen_lake.py:271: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"FrozenLake-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m     print_report(rewards, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[21], line 49\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mprint\u001b[39m(Q[state, :])\n\u001b[1;32m     48\u001b[0m action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(Q[state, :] \u001b[39m+\u001b[39m noise)\n\u001b[0;32m---> 49\u001b[0m state2, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     50\u001b[0m Qtarget \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m discount_factor \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmax(Q[state2, :])\n\u001b[1;32m     51\u001b[0m Q[state, action] \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mlearning_rate) \u001b[39m*\u001b[39m Q[state, action] \u001b[39m+\u001b[39m learning_rate \u001b[39m*\u001b[39m Qtarget\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bot 3 -- Build simple q-learning agent for FrozenLake\n",
    "\"\"\"\n",
    "\n",
    "from typing import List\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(0)  # make results reproducible\n",
    "np.random.seed(0)  # make results reproducible\n",
    "\n",
    "num_episodes = 4000\n",
    "discount_factor = 0.8\n",
    "learning_rate = 0.9\n",
    "report_interval = 500\n",
    "report = '100-ep Average: %.2f . Best 100-ep Average: %.2f . Average: %.2f ' \\\n",
    "         '(Episode %d)'\n",
    "\n",
    "\n",
    "def print_report(rewards: List, episode: int):\n",
    "    \"\"\"Print rewards report for current episode\n",
    "    - Average for last 100 episodes\n",
    "    - Best 100-episode average across all time\n",
    "    - Average for all episodes across time\n",
    "    \"\"\"\n",
    "    print(report % (\n",
    "        np.mean(rewards[-100:]),\n",
    "        max([np.mean(rewards[i:i+100]) for i in range(len(rewards) - 100)]),\n",
    "        np.mean(rewards),\n",
    "        episode))\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = gym.make('FrozenLake-v1')  # create the game\n",
    "    env.action_space.seed(0)  # make results reproducible\n",
    "    rewards = []\n",
    "\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        env.render()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            noise = np.random.random((1, env.action_space.n)) / (episode**2.)\n",
    "            \n",
    "            print(Q[state, :])\n",
    "            \n",
    "            action = np.argmax(Q[state, :] + noise)\n",
    "            state2, reward, done, _ = env.step(action)\n",
    "            Qtarget = reward + discount_factor * np.max(Q[state2, :])\n",
    "            Q[state, action] = (1-learning_rate) * Q[state, action] + learning_rate * Qtarget\n",
    "            episode_reward += reward\n",
    "            state = state2\n",
    "            if done:\n",
    "                rewards.append(episode_reward)\n",
    "                if episode % report_interval == 0:\n",
    "                    print_report(rewards, episode)\n",
    "                break\n",
    "    print_report(rewards, -1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 240\u001b[0m\n\u001b[1;32m    237\u001b[0m             reward_sum \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    238\u001b[0m             previous_processed_observations_image_vector \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m start()\n",
      "Cell \u001b[0;32mIn[37], line 185\u001b[0m, in \u001b[0;36mstart\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     env\u001b[39m.\u001b[39mrender()\n\u001b[0;32m--> 185\u001b[0m     processed_observations_image_vector, previous_processed_observations_image_vector \u001b[39m=\u001b[39m image_preprocess_observations(observation_image, previous_processed_observations_image_vector, input_dimensions)\n\u001b[1;32m    187\u001b[0m     \u001b[39m# predict probabilities from the model\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     up_probability \u001b[39m=\u001b[39m ((model\u001b[39m.\u001b[39mpredict(processed_observations_image_vector\u001b[39m.\u001b[39mreshape([\u001b[39m1\u001b[39m,processed_observations_image_vector\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]]), batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mflatten()))\n",
      "Cell \u001b[0;32mIn[37], line 34\u001b[0m, in \u001b[0;36mimage_preprocess_observations\u001b[0;34m(current_observation_image, previous_image_processed_observation, input_dimensions)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" convert the 210x160x3 uint8 frame into a 6400 float vector \"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m image_processed_observation \u001b[39m=\u001b[39m current_observation_image[\u001b[39m35\u001b[39m:\u001b[39m195\u001b[39m] \u001b[39m# crop\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m image_processed_observation \u001b[39m=\u001b[39m reduce_image_resolution(image_processed_observation)\n\u001b[1;32m     35\u001b[0m image_processed_observation \u001b[39m=\u001b[39m remove_image_color(image_processed_observation)\n\u001b[1;32m     36\u001b[0m image_processed_observation \u001b[39m=\u001b[39m remove_image_background(image_processed_observation)\n",
      "Cell \u001b[0;32mIn[37], line 17\u001b[0m, in \u001b[0;36mreduce_image_resolution\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreduce_image_resolution\u001b[39m(image):\n\u001b[1;32m     16\u001b[0m     \u001b[39m# Take only alternate pixels and halves the resolution of the image\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     \u001b[39mreturn\u001b[39;00m image[::\u001b[39m2\u001b[39;49m, ::\u001b[39m2\u001b[39;49m, :]\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import array_to_img, img_to_array, load_img\n",
    "\n",
    "from keras.layers import Dense, Flatten, Conv2D, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def reduce_image_resolution(image):\n",
    "    # Take only alternate pixels and halves the resolution of the image\n",
    "    return image[::2, ::2, :]\n",
    "\n",
    "def remove_image_color(image):\n",
    "    \"\"\"Convert all color (RGB is the third dimension in the image)\"\"\"\n",
    "    return image[:, :, 0]\n",
    "\n",
    "def remove_image_background(image):\n",
    "    image[image == 144] = 0\n",
    "    image[image == 109] = 0\n",
    "    return image\n",
    "def set_rest_to_one(image):\n",
    "    image[image != 0] = 1 # Set everything else (ball, paddles) to 1\n",
    "    return image\n",
    "\n",
    "def image_preprocess_observations(current_observation_image, previous_image_processed_observation, input_dimensions):\n",
    "    \"\"\" convert the 210x160x3 uint8 frame into a 6400 float vector \"\"\"\n",
    "    image_processed_observation = current_observation_image[35:195] # crop\n",
    "    image_processed_observation = reduce_image_resolution(image_processed_observation)\n",
    "    image_processed_observation = remove_image_color(image_processed_observation)\n",
    "    image_processed_observation = remove_image_background(image_processed_observation)\n",
    "    image_processed_observation = set_rest_to_one(image_processed_observation)\n",
    "    # Convert from 80 x 80 matrix to 1600 x 1 matrix\n",
    "    image_processed_observation = image_processed_observation.astype(np.float).ravel()\n",
    "\n",
    "    # subtract the previous frame from the current one\n",
    "    if previous_image_processed_observation is not None:\n",
    "        image_current_observation = image_processed_observation - previous_image_processed_observation\n",
    "    else:\n",
    "        current_observation_image = np.zeros(input_dimensions)\n",
    "    # update the previous_image_processed_observation\n",
    "    image_processed_observation = image_processed_observation\n",
    "    return current_observation_image, image_processed_observation\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def relu(vector):\n",
    "    vector[vector < 0] = 0\n",
    "    return vector\n",
    "\n",
    "def apply_neural_nets(observation_matrix, weights):\n",
    "    \"\"\" Based on the observation_matrix and weights, compute the new hidden layer values and the new output layer values\"\"\"\n",
    "    hidden_layer_values = np.dot(weights['1'], observation_matrix)\n",
    "    hidden_layer_values = relu(hidden_layer_values)\n",
    "    output_layer_values = np.dot(hidden_layer_values, weights['2'])\n",
    "    output_layer_values = sigmoid(output_layer_values)\n",
    "    return hidden_layer_values, output_layer_values\n",
    "\n",
    "def choose_action(probability):\n",
    "    random_value = np.random.uniform()\n",
    "    if random_value < probability:\n",
    "        return 2 # signifies upward movement in OpenAI gym pong implmentation\n",
    "    else:\n",
    "        return 3 # signifies downward movement in OpenAI gym pong implmentation\n",
    "\n",
    "def compute_gradient(gradient_log_p, hidden_layer_values, observation_values, weights):\n",
    "    \"\"\" Refer: http://neuralnetworksanddeeplearning.com/chap2.html\"\"\"\n",
    "    delta_L = gradient_log_p\n",
    "    dC_dw2 = np.dot(hidden_layer_values.T, delta_L).ravel()\n",
    "    delta_l2 = np.outer(delta_L, weights['2'])\n",
    "    delta_l2 = relu(delta_l2)\n",
    "    dC_dw1 = np.dot(delta_l2.T, observation_values)\n",
    "    return {\n",
    "        '1': dC_dw1,\n",
    "        '2': dC_dw2\n",
    "    }\n",
    "\n",
    "def update_weights(weights, expectation_g_squared, g_dict, decay_rate, learning_rate):\n",
    "    \"\"\" Refer: http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop\"\"\"\n",
    "    epsilon = 1e-5\n",
    "    for layer_name in weights.keys():\n",
    "        g = g_dict[layer_name]\n",
    "        expectation_g_squared[layer_name] = decay_rate * expectation_g_squared[layer_name] + (1 - decay_rate) * g**2\n",
    "        weights[layer_name] += (learning_rate * g)/(np.sqrt(expectation_g_squared[layer_name] + epsilon))\n",
    "        g_dict[layer_name] = np.zeros_like(weights[layer_name]) # reset batch gradient buffer\n",
    "\n",
    "def discount_rewards(rewards, gamma):\n",
    "    \"\"\" Actions taken 20 steps before the end result are less important to the overall result than an action taken a step ago.\n",
    "    This implements that logic by discounting the reward on previous actions based on how long ago they were taken\"\"\"\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, rewards.size)):\n",
    "        if rewards[t] != 0:\n",
    "            running_add = 0 # since this was a game boundary (pong specific!), reset the sum\n",
    "        running_add = running_add * gamma + rewards[t]\n",
    "        discounted_rewards[t] = running_add\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_with_rewards(gradient_log_p, episode_rewards, gamma):\n",
    "    \"\"\" discount the gradient with the normalized rewards \"\"\"\n",
    "    discounted_episode_rewards = discount_rewards(episode_rewards, gamma)\n",
    "    # standardize the rewards to be unit normal which helps to control the gradient estimator variance)\n",
    "    discounted_episode_rewards -= np.mean(discounted_episode_rewards)\n",
    "    discounted_episode_rewards /= np.std(discounted_episode_rewards)\n",
    "    return gradient_log_p * discounted_episode_rewards\n",
    "\n",
    "# Define the model in keras (WIP)\n",
    "def keras_learning_model(input_dim=80*80, model_type=1, resume=False):\n",
    "  model = Sequential()\n",
    "  number_of_inputs = 8\n",
    "  if model_type==0:\n",
    "    model.add(Reshape((1,80,80), input_shape=(input_dim,)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(200, activation = 'relu'))\n",
    "    model.add(Dense(number_of_inputs, activation='softmax'))\n",
    "    opt = RMSprop(lr=learning_rate)\n",
    "  else:\n",
    "    model.add(Reshape((1,80,80), input_shape=(input_dim,)))\n",
    "    # model.add(Conv2D(32, 9, 9, subsample=(4, 4), border_mode='same', activation='relu', init='he_uniform'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(number_of_inputs, activation='softmax'))\n",
    "    opt = Adam(lr=learning_rate)\n",
    "  model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "  if resume == True:\n",
    "    model.load_weights('pong_model_checkpoint.h5')\n",
    "  return model\n",
    "\n",
    "model = keras_learning_model()\n",
    "\n",
    "# The execution starts here\n",
    "def start():\n",
    "    env = gym.make(\"Pong-v0\", render_mode='human') # start the OpenAI gym pong environment\n",
    "    observation_image = env.reset() # get the image\n",
    "\n",
    "    # hyperparameters\n",
    "    episode_number = 0\n",
    "    batch_size = 10\n",
    "    gamma = 0.99 # discount factor for reward\n",
    "    decay_rate = 0.99\n",
    "    num_hidden_layer_neurons = 200\n",
    "    input_dimensions = 80 * 80\n",
    "    learning_rate = 0.001 # original value: 1e-4, adam optimizer used here in keras model. (Refer : https://keras.io/optimizers/)\n",
    "\n",
    "\n",
    "    #Script Parameters for keras\n",
    "    update_frequency = 1 # to decide frequency of update for the keras model parameters\n",
    "    resume = False # to load a previous checkpoint model weights to run again.\n",
    "    render = True # to render the OpenAI environment.\n",
    "    train_X = []\n",
    "    train_y = []\n",
    "\n",
    "    episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], []\n",
    "    xs, dlogps, drs, probs = [],[],[],[]\n",
    "\n",
    "    episode_number = 0\n",
    "    reward_sum = 0\n",
    "    running_reward = None\n",
    "    previous_processed_observations_image_vector = None\n",
    "\n",
    "    weights = {\n",
    "        '1': np.random.randn(num_hidden_layer_neurons, input_dimensions) / np.sqrt(input_dimensions),\n",
    "        '2': np.random.randn(num_hidden_layer_neurons) / np.sqrt(num_hidden_layer_neurons)\n",
    "    }\n",
    "\n",
    "    # To be used with rmsprop algorithm. Refer:(http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop)\n",
    "    expectation_g_squared = {}\n",
    "    g_dict = {}\n",
    "    for layer_name in weights.keys():\n",
    "        expectation_g_squared[layer_name] = np.zeros_like(weights[layer_name])\n",
    "        g_dict[layer_name] = np.zeros_like(weights[layer_name])\n",
    "\n",
    "\n",
    "    model = keras_learning_model() # compile the model.\n",
    "\n",
    "    while True:\n",
    "        env.render()\n",
    "        processed_observations_image_vector, previous_processed_observations_image_vector = image_preprocess_observations(observation_image, previous_processed_observations_image_vector, input_dimensions)\n",
    "\n",
    "        # predict probabilities from the model\n",
    "        up_probability = ((model.predict(processed_observations_image_vector.reshape([1,processed_observations_image_vector.shape[0]]), batch_size=1).flatten()))\n",
    "\n",
    "        episode_observations.append(processed_observations_image_vector)\n",
    "        episode_hidden_layer_values.append(hidden_layer_values)\n",
    "\n",
    "        action = choose_action(up_probability)\n",
    "\n",
    "        # Implement the chosen action. Get back the details from the enviroment after performing the action\n",
    "        observation_image, reward, done, info = env.step(action)\n",
    "\n",
    "        reward_sum += reward\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        # Refer: http://cs231n.github.io/neural-networks-2/#losses\n",
    "        fake_label = 1 if action == 2 else 0\n",
    "        loss_function_gradient = fake_label - up_probability\n",
    "        episode_gradient_log_ps.append(loss_function_gradient)\n",
    "\n",
    "        # check if an episode is finished\n",
    "        if done:\n",
    "            episode_number += 1\n",
    "\n",
    "            # combine the following values for the episode\n",
    "            episode_hidden_layer_values = np.vstack(episode_hidden_layer_values)\n",
    "            episode_observations = np.vstack(episode_observations)\n",
    "            episode_gradient_log_ps = np.vstack(episode_gradient_log_ps)\n",
    "            episode_rewards = np.vstack(episode_rewards)\n",
    "\n",
    "            # tweak the gradient of the log_ps based on the discounted rewards\n",
    "            episode_gradient_log_ps_discounted = discount_with_rewards(episode_gradient_log_ps, episode_rewards, gamma)\n",
    "\n",
    "            gradient = compute_gradient(\n",
    "              episode_gradient_log_ps_discounted,\n",
    "              episode_hidden_layer_values,\n",
    "              episode_observations,\n",
    "              weights\n",
    "            )\n",
    "\n",
    "            # sum the gradient after hitting the batch size\n",
    "            for layer_name in gradient:\n",
    "                g_dict[layer_name] += gradient[layer_name]\n",
    "\n",
    "            if episode_number % batch_size == 0:\n",
    "                update_weights(weights, expectation_g_squared, g_dict, decay_rate, learning_rate)\n",
    "\n",
    "            episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], [] # reset values\n",
    "            observation_image = env.reset() # reset openAI environment\n",
    "            running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "            print ('Resetting environment. Total Episode Reward: %f. Running Mean: %f' % (reward_sum, running_reward))\n",
    "            reward_sum = 0\n",
    "            previous_processed_observations_image_vector = None\n",
    "\n",
    "start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

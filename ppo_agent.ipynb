{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym imageio torchvision==0.15.1 torch==2.0.0 torchaudio==2.0.0 ale-py gymnasium==0.28.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym[atari] autorom[accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- starting environment 1 ----\n",
      "act 0 [[0.24607427418231964, 0.2610017955303192, 0.24890394508838654, 0.24401994049549103]]\n",
      "done False\n",
      "act 1 [[0.25154709815979004, 0.22577346861362457, 0.24970334768295288, 0.2729760408401489]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done False\n",
      "act 2 [[0.24789516627788544, 0.139862522482872, 0.2412540167570114, 0.3709883391857147]]\n",
      "done False\n",
      "act 3 [[0.2005818486213684, 0.04341711848974228, 0.1945118010044098, 0.5614892840385437]]\n",
      "done False\n",
      "act 0 [[0.1345239281654358, 0.004168695770204067, 0.1417335569858551, 0.7195737361907959]]\n",
      "done False\n",
      "act 3 [[0.04260402172803879, 0.0008143780287355185, 0.055215585976839066, 0.9013660550117493]]\n",
      "done False\n",
      "act 3 [[0.004798536188900471, 0.00024314771872013807, 0.01533783320337534, 0.9796205163002014]]\n",
      "done False\n",
      "act 3 [[0.005713850725442171, 7.650707266293466e-05, 0.011482235044240952, 0.9827273488044739]]\n",
      "done False\n",
      "act 0 [[0.38437148928642273, 0.00424599414691329, 0.08630974590778351, 0.5250726938247681]]\n",
      "done False\n",
      "act 0 [[0.9335028529167175, 0.04904400184750557, 0.017165854573249817, 0.0002873367920983583]]\n",
      "done False\n",
      "act 0 [[0.4462622106075287, 0.5531933307647705, 0.0005444622947834432, 6.250958084486058e-10]]\n",
      "done False\n",
      "act 1 [[0.004953577648848295, 0.9950461387634277, 3.9819281028030673e-07, 4.712645956073729e-19]]\n",
      "done False\n",
      "act 1 [[1.5139352399273776e-05, 0.999984860420227, 5.7004737735733e-11, 5.477649449973032e-30]]\n",
      "done False\n",
      "act 1 [[1.1964047530454991e-08, 1.0, 2.3980701288389607e-15, 1.3536543165377733e-42]]\n",
      "done False\n",
      "act 1 [[2.9245390831267315e-12, 1.0, 2.848206123161978e-20, 0.0]]\n",
      "done False\n",
      "act 1 [[2.1010853426967893e-16, 1.0, 1.2722927405703318e-25, 0.0]]\n",
      "done False\n",
      "act 1 [[4.2319626425721265e-21, 1.0, 5.07181472732904e-31, 0.0]]\n",
      "done False\n",
      "act 1 [[3.65553983470737e-26, 1.0, 3.832210235092546e-36, 0.0]]\n",
      "done False\n",
      "act 1 [[2.0060706823477663e-31, 1.0, 8.120104211223017e-41, 0.0]]\n",
      "done False\n",
      "act 1 [[8.025765883903865e-37, 1.0, 2.802596928649634e-45, 0.0]]\n",
      "done False\n",
      "act 1 [[2.3485762262083934e-42, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 2.1159606811304738e-43, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 3.6545164677572955e-29, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 1.876243604925776e-09, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 4.834887072902057e-25, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 2 [[0.0, 0.0, 1.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n",
      "done False\n",
      "act 1 [[0.0, 1.0, 0.0, 0.0]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 202\u001b[0m\n\u001b[1;32m    200\u001b[0m env_name \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mspec\u001b[39m.\u001b[39mid\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    201\u001b[0m agent \u001b[39m=\u001b[39m PPOAgent(env, env_name)\n\u001b[0;32m--> 202\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(training_episodes)\n\u001b[1;32m    203\u001b[0m agent\u001b[39m.\u001b[39mclose_session()\n\u001b[1;32m    205\u001b[0m \u001b[39m# free up memory for next iteration\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 150\u001b[0m, in \u001b[0;36mPPOAgent.train\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m    147\u001b[0m total_trad_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m trad\n\u001b[1;32m    148\u001b[0m total_nontrad_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nontrad\n\u001b[0;32m--> 150\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn((state, action, logits, combined_reward, next_state, done))\n\u001b[1;32m    152\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m    154\u001b[0m done \u001b[39m=\u001b[39m done \u001b[39mor\u001b[39;00m _\n",
      "Cell \u001b[0;32mIn[1], line 122\u001b[0m, in \u001b[0;36mPPOAgent.learn\u001b[0;34m(self, rollout)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    121\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> 122\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/optim/adam.py:344\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    341\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[1;32m    343\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m    345\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Categorical\n",
    "from utils import transform\n",
    "from fileIO import FileIO\n",
    "from utils import file_exists, write_history, get_last_history, compute_reward, decimal2\n",
    "\n",
    "# Define the PPO actor and critic neural network\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"CNN with 3 convolution layers\"\"\"\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(512, num_actions)\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(torch.prod(torch.tensor(o.size())))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # m = x[0].permute(1, 2, 0).numpy()\n",
    "        # plt.imshow(m)\n",
    "        # plt.axis('off')  # Optional: Turn off axis ticks and labels\n",
    "        # plt.show()\n",
    "        # return 'uuu'\n",
    "        # x = x.to(self.device)\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        fc_out = self.fc(conv_out)\n",
    "        return self.actor(fc_out), self.critic(fc_out)\n",
    "\n",
    "\n",
    "# Proximal Policy Optimization Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, env, env_name: str, lr=0.001, gamma=0.99, clip_epsilon=0.2,\n",
    "                 value_coef=0.5, entropy_coef=0.01):\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.name = env_name.lower()\n",
    "        self.input_shape = (3, 160, 160)\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.actor_critic = ActorCritic(self.input_shape, self.num_actions).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "        \n",
    "        # Define hyperparameters\n",
    "        self.gamma = gamma\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        self.resume = False\n",
    "        self.policy_path = f'saves/{self.name}/ppo_policy__.h5'\n",
    "        self.history_path = f'saves/{self.name}/ppo_history.csv'\n",
    "        self.log = ''\n",
    "        \n",
    "        # save time by retraining model from a known trained weights\n",
    "        if (file_exists(self.policy_path)):\n",
    "            self.resume = True\n",
    "            self.actor_critic.load_state_dict(torch.load(self.policy_path))\n",
    "            print('----- loaded saved weights ------')\n",
    "\n",
    "        # implement evaluation history so traning can be ran in batches to not crash system\n",
    "        self.current_episode =  get_last_history(self.history_path)\n",
    "\n",
    "    def act(self, state):\n",
    "        # state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            dist, value = self.actor_critic(state)\n",
    "            logits = F.softmax(dist, dim=-1)\n",
    "            action_probs = Categorical(logits)\n",
    "            action = action_probs.sample()\n",
    "            # self.log = f'{logits}, {action}'\n",
    "            \n",
    "        return action.item(), logits.tolist()\n",
    "\n",
    "    def learn(self, rollout):\n",
    "        states, actions, old_probs, rewards, next_states, dones = rollout\n",
    "\n",
    "        actions = torch.LongTensor([actions]).to(self.device)\n",
    "        rewards = torch.FloatTensor([rewards]).to(self.device)\n",
    "        old_probs = torch.FloatTensor([old_probs]).to(self.device)\n",
    "\n",
    "        dist, values = self.actor_critic(states)\n",
    "        _, next_values = self.actor_critic(next_states)\n",
    "\n",
    "        advantages = rewards + self.gamma * next_values * (1 - dones) - values.detach()\n",
    "        returns = rewards + self.gamma * next_values * (1 - dones)\n",
    "\n",
    "        prob_ratio = torch.exp(dist.squeeze(dim=0) - old_probs)\n",
    "\n",
    "        surrogate1 = prob_ratio * advantages\n",
    "        surrogate2 = torch.clamp(prob_ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "\n",
    "        actor_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "        critic_loss = F.smooth_l1_loss(values, returns)\n",
    "\n",
    "        # Calculate entropy using Categorical distribution\n",
    "        entropy = Categorical(probs=F.softmax(dist, dim=-1)).entropy().mean()\n",
    "\n",
    "        loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    \n",
    "    def train(self, num_episodes):\n",
    "        history_data = []\n",
    "        save_after = 10\n",
    "        for episode in range(num_episodes):\n",
    "            state = transform(self.env.reset()[0])\n",
    "            # state = self.env.reset()[0]\n",
    "\n",
    "            done = False\n",
    "            total_trad_reward = 0\n",
    "            total_nontrad_reward = 0\n",
    "            timestep = 0\n",
    "\n",
    "            while not done:\n",
    "                action, logits = self.act(state)\n",
    "                print('act', action, logits)\n",
    "                next_state, reward, done, _, __ = self.env.step(action)\n",
    "                next_state = transform(next_state)\n",
    "                \n",
    "                # get traditional and non-traditional reward\n",
    "                trad, nontrad = compute_reward(reward, done)\n",
    "                \n",
    "                combined_reward = trad + nontrad\n",
    "                total_trad_reward += trad\n",
    "                total_nontrad_reward += nontrad\n",
    "\n",
    "                self.learn((state, action, logits, combined_reward, next_state, done))\n",
    "\n",
    "                state = next_state\n",
    "                \n",
    "                done = done or _\n",
    "                print('done', done)\n",
    "                timestep += 1\n",
    "\n",
    "            total_nontrad_reward = decimal2(total_nontrad_reward)\n",
    "            total_reward = total_trad_reward + total_nontrad_reward\n",
    "            \n",
    "            print(f\"Episode {episode + 1} | Total Reward: {total_reward}, Traditional: {total_trad_reward}\")\n",
    "            \n",
    "            # persist history and policy after every 20 episodes\n",
    "            history_data.append(f\"{int(self.current_episode) + episode + 1}, {total_reward}, {total_trad_reward}, {total_nontrad_reward}, {timestep}, {self.log}\")\n",
    "            if ((episode + 1) % save_after == 0):\n",
    "                # Save the network weights\n",
    "                torch.save(self.actor_critic.state_dict(), self.policy_path)\n",
    "                \n",
    "                # write history\n",
    "                write_history(self.history_path, history_data)\n",
    "                history_data = []\n",
    "\n",
    "    def evaluate(self, num_episodes):\n",
    "        wins = 0\n",
    "        for episode in range(num_episodes):\n",
    "            state = transform(self.env.reset()[0])\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.act(state)[0]\n",
    "                next_state, reward, done, _, __ = self.env.step(action)\n",
    "                state = transform(next_state)\n",
    "                if reward > 0:\n",
    "                    wins += 1\n",
    "        \n",
    "        print(f'Wins - {wins}; Episodes - {num_episodes}; Average - {wins / num_episodes}') \n",
    "    \n",
    "    def close_session(self):\n",
    "        self.env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATARI Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Proxy Proximal Optimization Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Breakout-v4'\n",
    "# env_name = 'Berzerk-v4'\n",
    "training_batch = 1\n",
    "training_episodes = 10\n",
    "eval_episodes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(training_batch):\n",
    "  print(f'---- starting environment {i + 1} ----')\n",
    "  env = gym.make(env_name)\n",
    "  env_name = env.spec.id.split('-')[0]\n",
    "  agent = PPOAgent(env, env_name)\n",
    "  agent.train(training_episodes)\n",
    "  agent.close_session()\n",
    "  \n",
    "  # free up memory for next iteration\n",
    "  del env\n",
    "  del agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Trained PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "env_name = env.spec.id.split('-')[0]\n",
    "agent = PPOAgent(env, env_name)\n",
    "agent.evaluate(eval_episodes)\n",
    "agent.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

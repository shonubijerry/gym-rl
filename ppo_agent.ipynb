{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Categorical\n",
    "from utils import transform\n",
    "from fileIO import FileIO\n",
    "from utils import file_exists, write_history, get_last_history, compute_reward, decimal2\n",
    "\n",
    "# Define the PPO actor and critic neural network\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"CNN with 3 convolution layers\"\"\"\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(512, num_actions)\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(torch.prod(torch.tensor(o.size())))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # m = x[0].permute(1, 2, 0).numpy()\n",
    "        # plt.imshow(m)\n",
    "        # plt.axis('off')  # Optional: Turn off axis ticks and labels\n",
    "        # plt.show()\n",
    "        # return 'uuu'\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        fc_out = self.fc(conv_out)\n",
    "        return self.actor(fc_out), self.critic(fc_out)\n",
    "\n",
    "\n",
    "# Proximal Policy Optimization Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, env, env_name: str, lr=0.001, gamma=0.99, clip_epsilon=0.2,\n",
    "                 value_coef=0.5, entropy_coef=0.01):\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.name = env_name.lower()\n",
    "        self.input_shape = (3, 160, 160)\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.actor_critic = ActorCritic(self.input_shape, self.num_actions).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "        \n",
    "        # Define hyperparameters\n",
    "        self.gamma = gamma\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        self.resume = False\n",
    "        self.policy_path = f'saves/{self.name}/ppo_policy.h5'\n",
    "        self.history_path = f'saves/{self.name}/ppo_history.csv'\n",
    "        self.log = ''\n",
    "        \n",
    "        # save time by retraining model from a known trained weights\n",
    "        if (file_exists(self.policy_path)):\n",
    "            self.resume = True\n",
    "            self.actor_critic.load_state_dict(torch.load(self.policy_path))\n",
    "\n",
    "        # implement evaluation history so traning can be ran in batches to not crash system\n",
    "        self.current_episode =  get_last_history(self.history_path)\n",
    "\n",
    "    def act(self, state):\n",
    "        # state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            dist, value = self.actor_critic(state)\n",
    "            logits = F.softmax(dist, dim=-1)\n",
    "            action_probs = Categorical(logits)\n",
    "            action = action_probs.sample()\n",
    "            # self.log = f'{logits}, {action}'\n",
    "            \n",
    "        return action.item(), logits.tolist()\n",
    "\n",
    "    def learn(self, rollout):\n",
    "        states, actions, old_probs, rewards, next_states, dones = rollout\n",
    "\n",
    "        actions = torch.LongTensor([actions])\n",
    "        rewards = torch.FloatTensor([rewards])\n",
    "        old_probs = torch.FloatTensor([old_probs])\n",
    "\n",
    "        dist, values = self.actor_critic(states)\n",
    "        _, next_values = self.actor_critic(next_states)\n",
    "\n",
    "        advantages = rewards + self.gamma * next_values.detach() - values.detach()\n",
    "        returns = rewards + self.gamma * next_values * (1 - dones)\n",
    "\n",
    "        prob_ratio = torch.exp(dist.squeeze(dim=0) - old_probs)\n",
    "\n",
    "        surrogate1 = prob_ratio * advantages\n",
    "        surrogate2 = torch.clamp(prob_ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "\n",
    "        actor_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "        critic_loss = nn.MSELoss()(values, returns)\n",
    "\n",
    "        entropy = Categorical(probs=F.softmax(dist, dim=-1)).entropy().mean()  # Calculate entropy using Categorical distribution\n",
    "\n",
    "        loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def train(self, num_episodes):\n",
    "        history_data = []\n",
    "        save_after = 10\n",
    "        for episode in range(num_episodes):\n",
    "            state = transform(self.env.reset()[0])\n",
    "\n",
    "            done = False\n",
    "            total_trad_reward = 0\n",
    "            total_nontrad_reward = 0\n",
    "            timestep = 0\n",
    "\n",
    "            while not done:\n",
    "                action, logits = self.act(state)\n",
    "                next_state, reward, done, _, __ = self.env.step(action)\n",
    "                next_state = transform(next_state)\n",
    "                \n",
    "                # get traditional and non-traditional reward\n",
    "                trad, nontrad = compute_reward(reward, done)\n",
    "                \n",
    "                combined_reward = trad + nontrad\n",
    "                total_trad_reward += trad\n",
    "                total_nontrad_reward += nontrad\n",
    "\n",
    "                self.learn((state, action, logits, combined_reward, next_state, done))\n",
    "\n",
    "                state = next_state\n",
    "                \n",
    "                done = done or _\n",
    "                timestep += 1\n",
    "\n",
    "            total_nontrad_reward = decimal2(total_nontrad_reward)\n",
    "            total_reward = total_trad_reward + total_nontrad_reward\n",
    "            \n",
    "            print(f\"Episode {episode + 1} | Total Reward: {total_reward}\")\n",
    "            \n",
    "            # persist history and policy after every 20 episodes\n",
    "            history_data.append(f\"{int(self.current_episode) + episode + 1}, {total_reward}, {total_trad_reward}, {total_nontrad_reward}, {timestep}, {self.log}\")\n",
    "            if ((episode + 1) % save_after == 0):\n",
    "                # Save the network weights\n",
    "                torch.save(self.actor_critic.state_dict(), self.policy_path)\n",
    "                \n",
    "                # write history\n",
    "                write_history(self.history_path, history_data)\n",
    "                history_data = []\n",
    "\n",
    "    def evaluate(self, num_episodes):\n",
    "        wins = 0\n",
    "        for episode in range(num_episodes):\n",
    "            state = transform(self.env.reset()[0])\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.act(state)[0]\n",
    "                next_state, reward, done, _, __ = self.env.step(action)\n",
    "                state = transform(next_state)\n",
    "                if reward > 0:\n",
    "                    wins += 1\n",
    "        \n",
    "        print(f'Wins - {wins}; Episodes - {num_episodes}; Average - {wins / num_episodes}') \n",
    "    \n",
    "    def close_session(self):\n",
    "        self.env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATARI Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Proxy Proximal Optimization Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 160, 160])\n",
      "Episode 1 | Total Reward: 17.29\n",
      "torch.Size([1, 3, 160, 160])\n",
      "Episode 2 | Total Reward: 36.6\n",
      "torch.Size([1, 3, 160, 160])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m env_name \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mspec\u001b[39m.\u001b[39mid\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m agent \u001b[39m=\u001b[39m PPOAgent(env, env_name)\n\u001b[0;32m----> 5\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(\u001b[39m60\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m agent\u001b[39m.\u001b[39mclose_session()\n\u001b[1;32m      8\u001b[0m \u001b[39m# free up memory for next iteration\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 144\u001b[0m, in \u001b[0;36mPPOAgent.train\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m    141\u001b[0m total_trad_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m trad\n\u001b[1;32m    142\u001b[0m total_nontrad_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nontrad\n\u001b[0;32m--> 144\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn((state, action, logits, combined_reward, next_state, done))\n\u001b[1;32m    146\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m    148\u001b[0m done \u001b[39m=\u001b[39m done \u001b[39mor\u001b[39;00m _\n",
      "Cell \u001b[0;32mIn[7], line 98\u001b[0m, in \u001b[0;36mPPOAgent.learn\u001b[0;34m(self, rollout)\u001b[0m\n\u001b[1;32m     95\u001b[0m rewards \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor([rewards])\n\u001b[1;32m     96\u001b[0m old_probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor([old_probs])\n\u001b[0;32m---> 98\u001b[0m dist, values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor_critic(states)\n\u001b[1;32m     99\u001b[0m _, next_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor_critic(next_states)\n\u001b[1;32m    101\u001b[0m advantages \u001b[39m=\u001b[39m rewards \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m next_values\u001b[39m.\u001b[39mdetach() \u001b[39m-\u001b[39m values\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 45\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     39\u001b[0m     \u001b[39m# m = x[0].permute(1, 2, 0).numpy()\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[39m# plt.imshow(m)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[39m# plt.axis('off')  # Optional: Turn off axis ticks and labels\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[39m# plt.show()\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[39m# return 'uuu'\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     conv_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv(x)\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     fc_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(conv_out)\n\u001b[1;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor(fc_out), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic(fc_out)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "  env = gym.make(\"Breakout-v4\")\n",
    "  env_name = env.spec.id.split('-')[0]\n",
    "  agent = PPOAgent(env, env_name)\n",
    "  agent.train(60)\n",
    "  agent.close_session()\n",
    "  \n",
    "  # free up memory for next iteration\n",
    "  del env\n",
    "  del agent\n",
    "  \n",
    "  print(f'---- starting environment {i + 1} ----')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Trained PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Breakout-v4\")\n",
    "env_name = env.spec.id.split('-')[0]\n",
    "agent = PPOAgent(env, env_name)\n",
    "episodes = 20\n",
    "agent.evaluate(episodes)\n",
    "agent.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
